{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NN_hw2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3g9B1XjN74oX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 870
        },
        "outputId": "ea0b8523-8a18-4c82-c2a3-169335055088"
      },
      "source": [
        "! pip install ipdb"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ipdb\n",
            "  Downloading ipdb-0.13.9.tar.gz (16 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from ipdb) (57.4.0)\n",
            "Collecting ipython>=7.17.0\n",
            "  Downloading ipython-7.30.1-py3-none-any.whl (791 kB)\n",
            "\u001b[K     |████████████████████████████████| 791 kB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: toml>=0.10.2 in /usr/local/lib/python3.7/dist-packages (from ipdb) (0.10.2)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipdb) (4.4.2)\n",
            "Requirement already satisfied: jedi>=0.16 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.18.1)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (4.8.0)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (2.6.1)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (5.1.1)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.2.0)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.7.5)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.7/dist-packages (from ipython>=7.17.0->ipdb) (0.1.3)\n",
            "Collecting prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0\n",
            "  Downloading prompt_toolkit-3.0.24-py3-none-any.whl (374 kB)\n",
            "\u001b[K     |████████████████████████████████| 374 kB 59.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.16->ipython>=7.17.0->ipdb) (0.8.3)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.7/dist-packages (from pexpect>4.3->ipython>=7.17.0->ipdb) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=7.17.0->ipdb) (0.2.5)\n",
            "Building wheels for collected packages: ipdb\n",
            "  Building wheel for ipdb (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ipdb: filename=ipdb-0.13.9-py3-none-any.whl size=11648 sha256=861022cb8ba5abc90c726ae1d8973e672954ef3d7d00e2ea212c614cedf9855f\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/cd/cc/aaf92acae337a28fdd2aa4d632196a59745c8c39f76eaeed01\n",
            "Successfully built ipdb\n",
            "Installing collected packages: prompt-toolkit, ipython, ipdb\n",
            "  Attempting uninstall: prompt-toolkit\n",
            "    Found existing installation: prompt-toolkit 1.0.18\n",
            "    Uninstalling prompt-toolkit-1.0.18:\n",
            "      Successfully uninstalled prompt-toolkit-1.0.18\n",
            "  Attempting uninstall: ipython\n",
            "    Found existing installation: ipython 5.5.0\n",
            "    Uninstalling ipython-5.5.0:\n",
            "      Successfully uninstalled ipython-5.5.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "jupyter-console 5.2.0 requires prompt-toolkit<2.0.0,>=1.0.0, but you have prompt-toolkit 3.0.24 which is incompatible.\n",
            "google-colab 1.0.0 requires ipython~=5.5.0, but you have ipython 7.30.1 which is incompatible.\u001b[0m\n",
            "Successfully installed ipdb-0.13.9 ipython-7.30.1 prompt-toolkit-3.0.24\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "IPython",
                  "prompt_toolkit"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install torchmetrics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkxlDv93q8IV",
        "outputId": "f62f8b9d-a35e-4fa7-f468-79c15b09ac54"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchmetrics\n",
            "  Downloading torchmetrics-0.6.1-py3-none-any.whl (332 kB)\n",
            "\u001b[?25l\r\u001b[K     |█                               | 10 kB 23.5 MB/s eta 0:00:01\r\u001b[K     |██                              | 20 kB 28.3 MB/s eta 0:00:01\r\u001b[K     |███                             | 30 kB 29.1 MB/s eta 0:00:01\r\u001b[K     |████                            | 40 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |█████                           | 51 kB 31.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 61 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████                         | 71 kB 31.0 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 81 kB 30.8 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 92 kB 30.0 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 102 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 112 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████▉                    | 122 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 133 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 143 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 153 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 163 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 174 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 184 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▊             | 194 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 204 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 215 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 225 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 235 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 245 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 256 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▋      | 266 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 276 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 286 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 296 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 307 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 317 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 327 kB 30.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 332 kB 30.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (21.3)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.19.5)\n",
            "Requirement already satisfied: torch>=1.3.1 in /usr/local/lib/python3.7/dist-packages (from torchmetrics) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.3.1->torchmetrics) (3.10.0.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->torchmetrics) (3.0.6)\n",
            "Installing collected packages: torchmetrics\n",
            "Successfully installed torchmetrics-0.6.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from string import punctuation\n",
        "from collections import Counter\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torch.optim as optim\n",
        "import ipdb\n",
        "\n",
        "import re\n",
        "import torchmetrics\n",
        "from torchmetrics import F1\n",
        "from torchmetrics.functional import f1, recall\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2FQx1xEMRgU9"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd1Wlr3I75gc"
      },
      "source": [
        "pos_tweets = pd.read_csv('positive (1).csv', encoding='utf-8', sep=';', header=None,  names=[0,1,2,'text','tone',5,6,7,8,9,10,11])"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7rvRBNv375iq"
      },
      "source": [
        "neg_tweets = pd.read_csv('negative (1).csv', encoding='utf-8', sep=';', header=None, names=[0,1,2,'text','tone',5,6,7,8,9,10,11] )\n",
        "neg_tweets['tone'] = 0"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N03gIvPk99Ug",
        "outputId": "76926e88-eee7-40ac-b39a-5b355fe9438a"
      },
      "source": [
        "all_tweets_data = pos_tweets.append(neg_tweets)\n",
        "print(len(all_tweets_data))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "226834\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WHFweqho-CEf"
      },
      "source": [
        "tweets_data = shuffle(all_tweets_data[['text','tone']])[:100000]"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text):\n",
        "    res = re.sub(r'[^А-яЁё]', ' ', text)\n",
        "    res = res.lower()\n",
        "    return res"
      ],
      "metadata": {
        "id": "FcMBkt20W6Xl"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tweets_data['text'] = tweets_data.text.apply(preprocess)"
      ],
      "metadata": {
        "id": "e4nat3slGMhs"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, val_sentences = train_test_split(tweets_data, test_size=0.2)"
      ],
      "metadata": {
        "id": "mn8aIvsK8FBD"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = Counter()\n",
        "\n",
        "for text in tweets_data['text']:\n",
        "    text = text.split()\n",
        "    vocab.update(text)\n",
        "print('всего уникальных токенов:', len(vocab))\n",
        "\n",
        "filtered_vocab = set()\n",
        "for word in vocab:\n",
        "    if vocab[word] > 5:\n",
        "        filtered_vocab.add(word)\n",
        "print('уникальных токенов, встретившихся больше 5 раз:', len(filtered_vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRxk0UjmtnDP",
        "outputId": "f4eab854-083d-40f8-b1b9-5886f57f5d46"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "всего уникальных токенов: 106408\n",
            "уникальных токенов, встретившихся больше 5 раз: 14485\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#создаем словарь с индексами symbol2id, для спецсимвола паддинга дефолтный индекс - 0\n",
        "symbol2id = {'PAD':0}\n",
        "\n",
        "for symbol in filtered_vocab:\n",
        "    symbol2id[symbol] = len(symbol2id)\n",
        "\n",
        "#обратный словарь для того, чтобы раскодировать последовательность\n",
        "id2symbol = {i:symbol for symbol, i in symbol2id.items()}"
      ],
      "metadata": {
        "id": "ind0u7FdtnJH"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
        "DEVICE"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rb42qpPTvwW2",
        "outputId": "8b8d468e-ba40-42e0-8b86-c7e93cfbfeb1"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Dataset & DataLoader"
      ],
      "metadata": {
        "id": "2cjGFAXjw86B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetsDataset(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = torch.Tensor(dataset['tone'].values)\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self): #это обязательный метод, он должен уметь считать длину датасета\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index): #еще один обязательный метод. По индексу возвращает элемент выборки\n",
        "        tokens = self.dataset[index].split()\n",
        "        ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        return ids, y\n",
        "    \n",
        "#    def preprocess(self, text):\n",
        "#        tokens = text.lower().split()\n",
        "#        tokens = [token.strip(punctuation) for token in tokens]\n",
        "#        tokens = [token for token in tokens if token]\n",
        "#        return tokens\n",
        "\n",
        "    def collate_fn(self, batch): #этот метод можно реализовывать и отдельно,\n",
        "    # он понадобится для DataLoader во время итерации по батчам\n",
        "      ids, y = list(zip(*batch))\n",
        "      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n",
        "      #мы хотим применять BCELoss, он будет брать на вход predicted размера batch_size x 1 (так как для каждого семпла модель будет отдавать одно число), target размера batch_size x 1\n",
        "      y = torch.Tensor(y).to(self.device) # tuple ([1], [0], [1])  -> Tensor [[1.], [0.], [1.]] \n",
        "      return padded_ids, y"
      ],
      "metadata": {
        "id": "RzTBAFN-8FEc"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TweetsDataset(train_sentences, symbol2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "z0ainPpGxBkk"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch = next(iter(train_iterator))"
      ],
      "metadata": {
        "id": "escQtIEjAA9x"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataset = TweetsDataset(val_sentences, symbol2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "l-DyzgMHABA-"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch = next(iter(val_iterator))"
      ],
      "metadata": {
        "id": "KhwdLDi0Ab0N"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "hOwjU8EWWezk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=60, kernel_size=2, padding='same')\n",
        "self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3, padding='same')\n",
        "self.sec_conv = nn.Conv1d(in_channels=160, out_channels=40, kernel_size=3, padding='same')"
      ],
      "metadata": {
        "id": "SC8l6ABV1TDy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN(nn.Module):\n",
        "    \n",
        "    def __init__(self, vocab_size, embedding_dim):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.bigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=60, kernel_size=2, padding='same')\n",
        "        self.trigrams = nn.Conv1d(in_channels=embedding_dim, out_channels=100, kernel_size=3, padding='same')\n",
        "        self.sec_conv = nn.Conv1d(in_channels=160, out_channels=40, kernel_size=3, padding='same')\n",
        "        self.pooling = nn.MaxPool1d(kernel_size=2, stride=2)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.hidden = nn.Linear(in_features=40, out_features=1)\n",
        "        self.dropout = nn.Dropout(p=0.5)\n",
        "        self.out = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, word):\n",
        "        #batch_size x seq_len\n",
        "        embedded = self.embedding(word)\n",
        "        #batch_size x seq_len x embedding_dim\n",
        "        embedded = embedded.transpose(1,2)\n",
        "        #batch_size x embedding_dim x seq_len\n",
        "        feature_map_bigrams = self.dropout(self.relu(self.bigrams(embedded)))\n",
        "        #batch_size x filter_count2 x seq_len* \n",
        "        feature_map_trigrams = self.dropout(self.relu(self.trigrams(embedded)))\n",
        "        #batch_size x filter_count3 x seq_len*\n",
        "\n",
        "        #pooling1 = feature_map_bigrams.max(2)[0] \n",
        "        # batch_size x filter_count2\n",
        "        #pooling2 = feature_map_trigrams.max(2)[0]\n",
        "        # batch_size x filter_count3\n",
        "        concat = torch.cat((feature_map_bigrams, feature_map_trigrams), 1)\n",
        "        third_conv = self.dropout(self.pooling(self.relu(self.sec_conv(concat))))\n",
        "        pooling1 = third_conv.max(2)[0]\n",
        "        # batch _size x (filter_count2 + filter_count3)\n",
        "        logits = self.hidden(pooling1) \n",
        "        logits = self.out(self.dropout(logits))      \n",
        "        return logits"
      ],
      "metadata": {
        "id": "zz7RVfIcAb3x"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train"
      ],
      "metadata": {
        "id": "AanYLms6djqH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n",
        "\n",
        "    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n",
        "\n",
        "    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n",
        "        optimizer.zero_grad()  #обнуляем градиенты\n",
        "        preds = model(texts)  #прогоняем данные через модель\n",
        "        loss = criterion(preds, ys) #считаем значение функции потерь  \n",
        "        loss.backward() #считаем градиенты  \n",
        "        optimizer.step() #обновляем веса \n",
        "        epoch_loss += loss.item() #сохраняем значение функции потерь\n",
        "        if not (i + 1) % int(len(iterator)/5):\n",
        "            print(f'Train loss: {epoch_loss/i}')      \n",
        "    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке"
      ],
      "metadata": {
        "id": "QJnvSk5RWBh3"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    epoch_loss = 0\n",
        "    epoch_metric = 0\n",
        "    model.eval() \n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in enumerate(iterator):   \n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n",
        "            epoch_loss += loss.item()\n",
        "            batch_metric = f1(preds.round().long(), ys.long(), ignore_index=0)\n",
        "            epoch_metric += batch_metric\n",
        "\n",
        "            if not (i + 1) % int(len(iterator)/5):\n",
        "              print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n",
        "        \n",
        "    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке"
      ],
      "metadata": {
        "id": "lXBMt0HAWBkR"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def learning(n_epochs, model, optimizer, criterion):\n",
        "    for i in range(n_epochs):\n",
        "        print(f'\\nstarting Epoch {i}')\n",
        "        print('Training...')\n",
        "        epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "        losses.append(epoch_loss)\n",
        "        print('\\nEvaluating on train...')\n",
        "        f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "        f1s.append(f1_on_train.cpu())\n",
        "        print('\\nEvaluating on test...')\n",
        "        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "        losses_eval.append(epoch_loss_on_test)\n",
        "        f1s_eval.append(f1_on_test.cpu())"
      ],
      "metadata": {
        "id": "XKvQZ0Blgd6S"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN(len(symbol2id), 4)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0005)\n",
        "criterion = nn.BCELoss()  \n",
        "\n",
        "# веса модели и значения лосса храним там же, где и все остальные тензоры\n",
        "model = model.to(DEVICE)\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "O_jPiWe3Vuos"
      },
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "losses_eval = []\n",
        "f1s = []\n",
        "f1s_eval = []\n",
        "\n",
        "for i in range(30):\n",
        "    print(f'\\nstarting Epoch {i}')\n",
        "    print('Training...')\n",
        "    epoch_loss = train(model, train_iterator, optimizer, criterion)\n",
        "    losses.append(epoch_loss)\n",
        "    print('\\nEvaluating on train...')\n",
        "    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n",
        "    f1s.append(f1_on_train)\n",
        "    print('\\nEvaluating on test...')\n",
        "    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n",
        "    losses_eval.append(epoch_loss_on_test)\n",
        "    f1s_eval.append(f1_on_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RXM9WG-SVusP",
        "outputId": "30e47dbb-32f7-47cb-f14f-c00197c2cda7"
      },
      "execution_count": 151,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "starting Epoch 0\n",
            "Training...\n",
            "Train loss: 0.7660761943885258\n",
            "Train loss: 0.7357284919968967\n",
            "Train loss: 0.7252624210986224\n",
            "Train loss: 0.7193151288113352\n",
            "Train loss: 0.7155177802652926\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7457910222666604, Val f1: 0.017919911071658134\n",
            "Val loss: 0.7194051680893734, Val f1: 0.01705903373658657\n",
            "Val loss: 0.7110642343759537, Val f1: 0.0165686197578907\n",
            "Val loss: 0.7073793189000275, Val f1: 0.01682894676923752\n",
            "Val loss: 0.7048568306742488, Val f1: 0.0160882156342268\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9262000521024069, Val f1: 0.020141126587986946\n",
            "Val loss: 0.793457567691803, Val f1: 0.015261885710060596\n",
            "Val loss: 0.7569042064926841, Val f1: 0.016808820888400078\n",
            "Val loss: 0.7403485298156738, Val f1: 0.01718178577721119\n",
            "Val loss: 0.7309328411754809, Val f1: 0.01534013357013464\n",
            "\n",
            "starting Epoch 1\n",
            "Training...\n",
            "Train loss: 0.7502446728093284\n",
            "Train loss: 0.7232050135217863\n",
            "Train loss: 0.7143856950781562\n",
            "Train loss: 0.7102252739970967\n",
            "Train loss: 0.707700131712733\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.743188453572137, Val f1: 0.10475891083478928\n",
            "Val loss: 0.7169439607653124, Val f1: 0.10059688240289688\n",
            "Val loss: 0.7085323889147151, Val f1: 0.10017573833465576\n",
            "Val loss: 0.7044534248820806, Val f1: 0.10296766459941864\n",
            "Val loss: 0.7020892347838428, Val f1: 0.10302013903856277\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9233277440071106, Val f1: 0.12991231679916382\n",
            "Val loss: 0.791039594582149, Val f1: 0.11627274006605148\n",
            "Val loss: 0.754602394320748, Val f1: 0.1075456440448761\n",
            "Val loss: 0.7380975524584452, Val f1: 0.10375550389289856\n",
            "Val loss: 0.7285950309351871, Val f1: 0.10171326249837875\n",
            "\n",
            "starting Epoch 2\n",
            "Training...\n",
            "Train loss: 0.7474965325423649\n",
            "Train loss: 0.7213963681253893\n",
            "Train loss: 0.7129451998255469\n",
            "Train loss: 0.7084202251191867\n",
            "Train loss: 0.7058817583161432\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7397524160998208, Val f1: 0.38828712701797485\n",
            "Val loss: 0.7147016813015116, Val f1: 0.3709653317928314\n",
            "Val loss: 0.7065061520446431, Val f1: 0.3687073886394501\n",
            "Val loss: 0.702400933887999, Val f1: 0.36954864859580994\n",
            "Val loss: 0.6999877288534835, Val f1: 0.3676643371582031\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9203671415646871, Val f1: 0.4862166941165924\n",
            "Val loss: 0.7887617094176156, Val f1: 0.4269704222679138\n",
            "Val loss: 0.7528060837225481, Val f1: 0.4077797830104828\n",
            "Val loss: 0.736374843120575, Val f1: 0.39340513944625854\n",
            "Val loss: 0.7269035169952794, Val f1: 0.3807023763656616\n",
            "\n",
            "starting Epoch 3\n",
            "Training...\n",
            "Train loss: 0.7475354841777256\n",
            "Train loss: 0.7201474391180893\n",
            "Train loss: 0.7109178683974526\n",
            "Train loss: 0.7063637981980534\n",
            "Train loss: 0.7038143285222955\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7400772316115243, Val f1: 0.27621990442276\n",
            "Val loss: 0.7142162919044495, Val f1: 0.261932909488678\n",
            "Val loss: 0.7062032385305925, Val f1: 0.2591893970966339\n",
            "Val loss: 0.7021449915433334, Val f1: 0.25755342841148376\n",
            "Val loss: 0.6997807340042012, Val f1: 0.2576170265674591\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.920477012793223, Val f1: 0.33210957050323486\n",
            "Val loss: 0.7886008790561131, Val f1: 0.287271648645401\n",
            "Val loss: 0.7525255625898187, Val f1: 0.2738661766052246\n",
            "Val loss: 0.7360899686813355, Val f1: 0.2662450075149536\n",
            "Val loss: 0.7266182930845964, Val f1: 0.26191288232803345\n",
            "\n",
            "starting Epoch 4\n",
            "Training...\n",
            "Train loss: 0.7427173342023577\n",
            "Train loss: 0.7166101891418983\n",
            "Train loss: 0.708671512928876\n",
            "Train loss: 0.7046440294233419\n",
            "Train loss: 0.7021701891680021\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7405146360397339, Val f1: 0.22037477791309357\n",
            "Val loss: 0.7142721209032782, Val f1: 0.2154308557510376\n",
            "Val loss: 0.7063373679464514, Val f1: 0.21388691663742065\n",
            "Val loss: 0.7020437828565048, Val f1: 0.2126879245042801\n",
            "Val loss: 0.6996423996783592, Val f1: 0.2131066769361496\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9204845825831095, Val f1: 0.28561830520629883\n",
            "Val loss: 0.7884258287293571, Val f1: 0.23705187439918518\n",
            "Val loss: 0.7522592652927745, Val f1: 0.22359178960323334\n",
            "Val loss: 0.7358322739601135, Val f1: 0.2175077348947525\n",
            "Val loss: 0.7264519992627596, Val f1: 0.21292877197265625\n",
            "\n",
            "starting Epoch 5\n",
            "Training...\n",
            "Train loss: 0.7408425467354911\n",
            "Train loss: 0.715488339292592\n",
            "Train loss: 0.707563342018561\n",
            "Train loss: 0.7036680801440094\n",
            "Train loss: 0.7014305897661157\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7392201466219765, Val f1: 0.2716628611087799\n",
            "Val loss: 0.7136002265173813, Val f1: 0.26569393277168274\n",
            "Val loss: 0.7053067697720095, Val f1: 0.2636095881462097\n",
            "Val loss: 0.701301652496144, Val f1: 0.26145830750465393\n",
            "Val loss: 0.6989457486449061, Val f1: 0.25991091132164\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9194624423980713, Val f1: 0.36442551016807556\n",
            "Val loss: 0.7877055065972465, Val f1: 0.30540937185287476\n",
            "Val loss: 0.7516827962615273, Val f1: 0.28246015310287476\n",
            "Val loss: 0.735251518090566, Val f1: 0.2706020176410675\n",
            "Val loss: 0.7257880254795677, Val f1: 0.26803168654441833\n",
            "\n",
            "starting Epoch 6\n",
            "Training...\n",
            "Train loss: 0.7412430473736354\n",
            "Train loss: 0.7143509819589812\n",
            "Train loss: 0.7059930427507921\n",
            "Train loss: 0.7018948136749914\n",
            "Train loss: 0.6992803892573795\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7377110847405025, Val f1: 0.3717506527900696\n",
            "Val loss: 0.711862276340353, Val f1: 0.3581481873989105\n",
            "Val loss: 0.7039993486621163, Val f1: 0.3531537353992462\n",
            "Val loss: 0.6999492190651975, Val f1: 0.3503461182117462\n",
            "Val loss: 0.6973619839629611, Val f1: 0.351479172706604\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9175628622372946, Val f1: 0.48131099343299866\n",
            "Val loss: 0.7860437631607056, Val f1: 0.40159133076667786\n",
            "Val loss: 0.7500942891294305, Val f1: 0.37970855832099915\n",
            "Val loss: 0.7336758852005005, Val f1: 0.3657679855823517\n",
            "Val loss: 0.7242901419338427, Val f1: 0.3614855110645294\n",
            "\n",
            "starting Epoch 7\n",
            "Training...\n",
            "Train loss: 0.7399044420037951\n",
            "Train loss: 0.7143549138102038\n",
            "Train loss: 0.7062602652744814\n",
            "Train loss: 0.7018926628565384\n",
            "Train loss: 0.6994860003123412\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7379494181701115, Val f1: 0.3196227550506592\n",
            "Val loss: 0.7124102485590967, Val f1: 0.31011199951171875\n",
            "Val loss: 0.7043198550289328, Val f1: 0.3062117397785187\n",
            "Val loss: 0.700038922035088, Val f1: 0.3071357011795044\n",
            "Val loss: 0.6974433515522931, Val f1: 0.3091281056404114\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9176279505093893, Val f1: 0.41564521193504333\n",
            "Val loss: 0.7861403397151402, Val f1: 0.3507882058620453\n",
            "Val loss: 0.7502128644423052, Val f1: 0.33252033591270447\n",
            "Val loss: 0.7338872989018758, Val f1: 0.31948548555374146\n",
            "Val loss: 0.7245245387679652, Val f1: 0.3120081424713135\n",
            "\n",
            "starting Epoch 8\n",
            "Training...\n",
            "Train loss: 0.7388068011828831\n",
            "Train loss: 0.7127646125596145\n",
            "Train loss: 0.7040199908343229\n",
            "Train loss: 0.7004941439224501\n",
            "Train loss: 0.6981532138747137\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7356076836585999, Val f1: 0.35741814970970154\n",
            "Val loss: 0.7099422401395338, Val f1: 0.3539786636829376\n",
            "Val loss: 0.7023141343485225, Val f1: 0.34734663367271423\n",
            "Val loss: 0.6983668198019771, Val f1: 0.34673693776130676\n",
            "Val loss: 0.6960445352502771, Val f1: 0.3458484411239624\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9153517484664917, Val f1: 0.45371201634407043\n",
            "Val loss: 0.7841473306928363, Val f1: 0.38652488589286804\n",
            "Val loss: 0.7483583255247637, Val f1: 0.3688706159591675\n",
            "Val loss: 0.7321319778760275, Val f1: 0.3566850423812866\n",
            "Val loss: 0.722903311252594, Val f1: 0.3509920537471771\n",
            "\n",
            "starting Epoch 9\n",
            "Training...\n",
            "Train loss: 0.7370247713157109\n",
            "Train loss: 0.7115134781804578\n",
            "Train loss: 0.7036032988266512\n",
            "Train loss: 0.6994039537542958\n",
            "Train loss: 0.6970151243983088\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7348648863179343, Val f1: 0.45585107803344727\n",
            "Val loss: 0.7087176709339537, Val f1: 0.43953484296798706\n",
            "Val loss: 0.7005059163678776, Val f1: 0.4356328248977661\n",
            "Val loss: 0.6966506273059522, Val f1: 0.43256089091300964\n",
            "Val loss: 0.6942391798302934, Val f1: 0.4313333332538605\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9131425619125366, Val f1: 0.5700402855873108\n",
            "Val loss: 0.7824051295007978, Val f1: 0.48738807439804077\n",
            "Val loss: 0.7468422976407137, Val f1: 0.46230414509773254\n",
            "Val loss: 0.730676547686259, Val f1: 0.44630616903305054\n",
            "Val loss: 0.7214228291260568, Val f1: 0.43882444500923157\n",
            "\n",
            "starting Epoch 10\n",
            "Training...\n",
            "Train loss: 0.7365251651832035\n",
            "Train loss: 0.7108088259039254\n",
            "Train loss: 0.7023558562452142\n",
            "Train loss: 0.6985358201851279\n",
            "Train loss: 0.695946044213063\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7329966425895691, Val f1: 0.5085846185684204\n",
            "Val loss: 0.7071484779489452, Val f1: 0.4945695698261261\n",
            "Val loss: 0.6992573304609819, Val f1: 0.48601168394088745\n",
            "Val loss: 0.6952196672811346, Val f1: 0.4839639961719513\n",
            "Val loss: 0.6929174621362943, Val f1: 0.4817343056201935\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9112631678581238, Val f1: 0.6489794850349426\n",
            "Val loss: 0.7808094791003636, Val f1: 0.5516998171806335\n",
            "Val loss: 0.7453432462432168, Val f1: 0.5248683094978333\n",
            "Val loss: 0.7292706370353699, Val f1: 0.5074089169502258\n",
            "Val loss: 0.7200544131429572, Val f1: 0.4972722828388214\n",
            "\n",
            "starting Epoch 11\n",
            "Training...\n",
            "Train loss: 0.7355311810970306\n",
            "Train loss: 0.7086994730193039\n",
            "Train loss: 0.701345603574406\n",
            "Train loss: 0.6975025645757126\n",
            "Train loss: 0.6949766536016722\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7314947332654681, Val f1: 0.4963037073612213\n",
            "Val loss: 0.7061066853589025, Val f1: 0.4838179349899292\n",
            "Val loss: 0.698036708615043, Val f1: 0.47995245456695557\n",
            "Val loss: 0.69409274852882, Val f1: 0.4787593483924866\n",
            "Val loss: 0.6917729788535351, Val f1: 0.4757585823535919\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9101632833480835, Val f1: 0.6337854266166687\n",
            "Val loss: 0.7797466090747288, Val f1: 0.5391563773155212\n",
            "Val loss: 0.7443556893955577, Val f1: 0.5136794447898865\n",
            "Val loss: 0.728347380956014, Val f1: 0.4989568293094635\n",
            "Val loss: 0.7190846518466347, Val f1: 0.4879411458969116\n",
            "\n",
            "starting Epoch 12\n",
            "Training...\n",
            "Train loss: 0.7329269094126565\n",
            "Train loss: 0.7085218758418642\n",
            "Train loss: 0.7001741840080782\n",
            "Train loss: 0.6962407716250015\n",
            "Train loss: 0.693982038948987\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7330168528216225, Val f1: 0.39911696314811707\n",
            "Val loss: 0.7071278382991922, Val f1: 0.38232529163360596\n",
            "Val loss: 0.6989672834222967, Val f1: 0.38165634870529175\n",
            "Val loss: 0.6948494668734275, Val f1: 0.38058093190193176\n",
            "Val loss: 0.692338041357092, Val f1: 0.3790305554866791\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9111330111821493, Val f1: 0.5074322819709778\n",
            "Val loss: 0.7802645564079285, Val f1: 0.4317726194858551\n",
            "Val loss: 0.7446475354107943, Val f1: 0.40795573592185974\n",
            "Val loss: 0.7286497990290324, Val f1: 0.3929442763328552\n",
            "Val loss: 0.7195186175798115, Val f1: 0.38741353154182434\n",
            "\n",
            "starting Epoch 13\n",
            "Training...\n",
            "Train loss: 0.7354769153254372\n",
            "Train loss: 0.7093858287252229\n",
            "Train loss: 0.7008556384931911\n",
            "Train loss: 0.6965833932666455\n",
            "Train loss: 0.6936873294211723\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7299542214189257, Val f1: 0.4966132342815399\n",
            "Val loss: 0.7045300931766115, Val f1: 0.48113110661506653\n",
            "Val loss: 0.6964527002789758, Val f1: 0.4750734269618988\n",
            "Val loss: 0.692591648990825, Val f1: 0.47221940755844116\n",
            "Val loss: 0.6901559443087191, Val f1: 0.4708283841609955\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9085001349449158, Val f1: 0.6157671809196472\n",
            "Val loss: 0.7781428694725037, Val f1: 0.5310121178627014\n",
            "Val loss: 0.7426855130629106, Val f1: 0.5063402652740479\n",
            "Val loss: 0.7266677419344584, Val f1: 0.4914575219154358\n",
            "Val loss: 0.71749809854909, Val f1: 0.4822706878185272\n",
            "\n",
            "starting Epoch 14\n",
            "Training...\n",
            "Train loss: 0.7317835347993034\n",
            "Train loss: 0.7074421644210815\n",
            "Train loss: 0.6991398835724051\n",
            "Train loss: 0.6948916083675319\n",
            "Train loss: 0.6925252579353951\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7293609082698822, Val f1: 0.4804418981075287\n",
            "Val loss: 0.7038821002532696, Val f1: 0.46092820167541504\n",
            "Val loss: 0.695947221734307, Val f1: 0.4538284242153168\n",
            "Val loss: 0.6916878819465637, Val f1: 0.4534858465194702\n",
            "Val loss: 0.6893947873566602, Val f1: 0.4530731439590454\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9078098932902018, Val f1: 0.5928407311439514\n",
            "Val loss: 0.7774295381137303, Val f1: 0.5074042677879333\n",
            "Val loss: 0.7419429529796947, Val f1: 0.4832433760166168\n",
            "Val loss: 0.7260141571362814, Val f1: 0.4686904549598694\n",
            "Val loss: 0.7168632275179813, Val f1: 0.46146267652511597\n",
            "\n",
            "starting Epoch 15\n",
            "Training...\n",
            "Train loss: 0.7309324358190809\n",
            "Train loss: 0.7056808101719824\n",
            "Train loss: 0.6979719549417496\n",
            "Train loss: 0.6932284347081589\n",
            "Train loss: 0.6907310759699022\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7265764474868774, Val f1: 0.5393157601356506\n",
            "Val loss: 0.7011380750557472, Val f1: 0.5236480832099915\n",
            "Val loss: 0.693405958739194, Val f1: 0.5151669979095459\n",
            "Val loss: 0.6893464114706395, Val f1: 0.5124281644821167\n",
            "Val loss: 0.6872017858801661, Val f1: 0.5100573897361755\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9048967361450195, Val f1: 0.6842465400695801\n",
            "Val loss: 0.775017363684518, Val f1: 0.5831788182258606\n",
            "Val loss: 0.7397425933317705, Val f1: 0.552123486995697\n",
            "Val loss: 0.7238497932751974, Val f1: 0.5349664688110352\n",
            "Val loss: 0.71477788059335, Val f1: 0.52447110414505\n",
            "\n",
            "starting Epoch 16\n",
            "Training...\n",
            "Train loss: 0.7289451956748962\n",
            "Train loss: 0.7041380878152519\n",
            "Train loss: 0.6956975297494368\n",
            "Train loss: 0.6919767129219184\n",
            "Train loss: 0.6897276704375809\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7264850267342159, Val f1: 0.5080434083938599\n",
            "Val loss: 0.7015878537605549, Val f1: 0.49033546447753906\n",
            "Val loss: 0.6937867958437313, Val f1: 0.48741722106933594\n",
            "Val loss: 0.6896700161998555, Val f1: 0.4863245189189911\n",
            "Val loss: 0.6871758651089024, Val f1: 0.48851141333580017\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.904615064462026, Val f1: 0.6544984579086304\n",
            "Val loss: 0.7748779654502869, Val f1: 0.5574943423271179\n",
            "Val loss: 0.7396185452287848, Val f1: 0.5280951261520386\n",
            "Val loss: 0.7238105018933614, Val f1: 0.5116789937019348\n",
            "Val loss: 0.7147513157442996, Val f1: 0.501581609249115\n",
            "\n",
            "starting Epoch 17\n",
            "Training...\n",
            "Train loss: 0.7287907344954354\n",
            "Train loss: 0.7046668036230679\n",
            "Train loss: 0.6964792500842701\n",
            "Train loss: 0.6921346086566731\n",
            "Train loss: 0.689255439751857\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7222486478941781, Val f1: 0.6027026176452637\n",
            "Val loss: 0.6976181165925388, Val f1: 0.5791828036308289\n",
            "Val loss: 0.6898944621736353, Val f1: 0.5715653300285339\n",
            "Val loss: 0.6857130830570802, Val f1: 0.570527970790863\n",
            "Val loss: 0.6835397860488376, Val f1: 0.5672054886817932\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.9003194769223531, Val f1: 0.7458038330078125\n",
            "Val loss: 0.7712077839033944, Val f1: 0.6399551033973694\n",
            "Val loss: 0.7362695661458102, Val f1: 0.6076273322105408\n",
            "Val loss: 0.7205902218818665, Val f1: 0.5880712270736694\n",
            "Val loss: 0.7114765895040411, Val f1: 0.5765185952186584\n",
            "\n",
            "starting Epoch 18\n",
            "Training...\n",
            "Train loss: 0.7269226738384792\n",
            "Train loss: 0.7012889344116737\n",
            "Train loss: 0.6939344392581419\n",
            "Train loss: 0.690228974415084\n",
            "Train loss: 0.6874571362057248\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7201941992555346, Val f1: 0.6107812523841858\n",
            "Val loss: 0.6963163860912981, Val f1: 0.5863919854164124\n",
            "Val loss: 0.6888164403763685, Val f1: 0.5769409537315369\n",
            "Val loss: 0.6847853973760443, Val f1: 0.5745893120765686\n",
            "Val loss: 0.682412518037332, Val f1: 0.574363648891449\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.899090309937795, Val f1: 0.7633492350578308\n",
            "Val loss: 0.7701043145997184, Val f1: 0.652291476726532\n",
            "Val loss: 0.7352364984425631, Val f1: 0.6170876622200012\n",
            "Val loss: 0.7195898016293844, Val f1: 0.5989544987678528\n",
            "Val loss: 0.7105529810252943, Val f1: 0.5869068503379822\n",
            "\n",
            "starting Epoch 19\n",
            "Training...\n",
            "Train loss: 0.7231973239353725\n",
            "Train loss: 0.7007517547442995\n",
            "Train loss: 0.6927797834981572\n",
            "Train loss: 0.6887940309815488\n",
            "Train loss: 0.6866715469875851\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7208810533796038, Val f1: 0.6272256970405579\n",
            "Val loss: 0.6956209088193959, Val f1: 0.6076647043228149\n",
            "Val loss: 0.6874188130552118, Val f1: 0.6020369529724121\n",
            "Val loss: 0.6833708296387883, Val f1: 0.599983811378479\n",
            "Val loss: 0.6811281845376298, Val f1: 0.5964712500572205\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8977827628453573, Val f1: 0.795423686504364\n",
            "Val loss: 0.7689502835273743, Val f1: 0.6817950010299683\n",
            "Val loss: 0.7341813770207491, Val f1: 0.6418223977088928\n",
            "Val loss: 0.7185318787892659, Val f1: 0.6218406558036804\n",
            "Val loss: 0.7094602051534151, Val f1: 0.6102404594421387\n",
            "\n",
            "starting Epoch 20\n",
            "Training...\n",
            "Train loss: 0.7257514340536935\n",
            "Train loss: 0.7003675411487448\n",
            "Train loss: 0.6919295733625238\n",
            "Train loss: 0.6879002108412274\n",
            "Train loss: 0.6858893318756206\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7181339859962463, Val f1: 0.6157965064048767\n",
            "Val loss: 0.6936908405402611, Val f1: 0.5950294733047485\n",
            "Val loss: 0.6861754818396135, Val f1: 0.5878633260726929\n",
            "Val loss: 0.6821163973565829, Val f1: 0.5862537026405334\n",
            "Val loss: 0.6795885852865271, Val f1: 0.5846360921859741\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8959370255470276, Val f1: 0.7752823829650879\n",
            "Val loss: 0.7672843251909528, Val f1: 0.6674684882164001\n",
            "Val loss: 0.7324990467591719, Val f1: 0.6309347152709961\n",
            "Val loss: 0.7169227321942647, Val f1: 0.6099646091461182\n",
            "Val loss: 0.7079641034728602, Val f1: 0.5986979007720947\n",
            "\n",
            "starting Epoch 21\n",
            "Training...\n",
            "Train loss: 0.7217679364340646\n",
            "Train loss: 0.6975279244883307\n",
            "Train loss: 0.6901176084171642\n",
            "Train loss: 0.68588739734585\n",
            "Train loss: 0.6831136297535252\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.71687969991139, Val f1: 0.6327861547470093\n",
            "Val loss: 0.6908140696328262, Val f1: 0.6161770820617676\n",
            "Val loss: 0.6831178367137909, Val f1: 0.6106529831886292\n",
            "Val loss: 0.679469243954804, Val f1: 0.605370283126831\n",
            "Val loss: 0.6770816268147649, Val f1: 0.603919267654419\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8928162058194479, Val f1: 0.8016276359558105\n",
            "Val loss: 0.7647325566836766, Val f1: 0.685452938079834\n",
            "Val loss: 0.7301465435461565, Val f1: 0.6472718119621277\n",
            "Val loss: 0.7146156112353007, Val f1: 0.6264938116073608\n",
            "Val loss: 0.7057400000722784, Val f1: 0.6155823469161987\n",
            "\n",
            "starting Epoch 22\n",
            "Training...\n",
            "Train loss: 0.7220813930034637\n",
            "Train loss: 0.6974962518132967\n",
            "Train loss: 0.6894826685840433\n",
            "Train loss: 0.6860306465019614\n",
            "Train loss: 0.6833035744525291\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7150810360908508, Val f1: 0.6164182424545288\n",
            "Val loss: 0.6901699016834127, Val f1: 0.5940950512886047\n",
            "Val loss: 0.6829216629266739, Val f1: 0.585254430770874\n",
            "Val loss: 0.6790829189753128, Val f1: 0.5836591720581055\n",
            "Val loss: 0.6769586108826302, Val f1: 0.5814300179481506\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8924016356468201, Val f1: 0.7713618278503418\n",
            "Val loss: 0.764355080468314, Val f1: 0.6635947227478027\n",
            "Val loss: 0.7296912995251742, Val f1: 0.625664472579956\n",
            "Val loss: 0.7142519434293111, Val f1: 0.6063818335533142\n",
            "Val loss: 0.7055071686443529, Val f1: 0.5931912660598755\n",
            "\n",
            "starting Epoch 23\n",
            "Training...\n",
            "Train loss: 0.7218897385256631\n",
            "Train loss: 0.6945586533382021\n",
            "Train loss: 0.6867752779613842\n",
            "Train loss: 0.6829836328150862\n",
            "Train loss: 0.6806624475363139\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7129230030945369, Val f1: 0.6125980019569397\n",
            "Val loss: 0.6876228677815405, Val f1: 0.5930519700050354\n",
            "Val loss: 0.680379335175861, Val f1: 0.5861809849739075\n",
            "Val loss: 0.6765250058497413, Val f1: 0.5827429890632629\n",
            "Val loss: 0.6741401174583951, Val f1: 0.5790254473686218\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8891959389050802, Val f1: 0.7667301297187805\n",
            "Val loss: 0.7613312005996704, Val f1: 0.6606196761131287\n",
            "Val loss: 0.7268104607408697, Val f1: 0.6228695511817932\n",
            "Val loss: 0.7115254799524943, Val f1: 0.6039935946464539\n",
            "Val loss: 0.7028972506523132, Val f1: 0.5914836525917053\n",
            "\n",
            "starting Epoch 24\n",
            "Training...\n",
            "Train loss: 0.7173016539641789\n",
            "Train loss: 0.6929079561397947\n",
            "Train loss: 0.6852471842007204\n",
            "Train loss: 0.6814312510571238\n",
            "Train loss: 0.6797145170134467\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7112108256135669, Val f1: 0.6873103380203247\n",
            "Val loss: 0.6866270694239386, Val f1: 0.6603481769561768\n",
            "Val loss: 0.6785322590307756, Val f1: 0.6551466584205627\n",
            "Val loss: 0.6746597310244027, Val f1: 0.6513533592224121\n",
            "Val loss: 0.6725724846930117, Val f1: 0.6471969485282898\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.887570301691691, Val f1: 0.8406704068183899\n",
            "Val loss: 0.7601895332336426, Val f1: 0.7228817939758301\n",
            "Val loss: 0.7259496342052113, Val f1: 0.6876450181007385\n",
            "Val loss: 0.7105600595474243, Val f1: 0.6677836179733276\n",
            "Val loss: 0.7017853260040283, Val f1: 0.6559425592422485\n",
            "\n",
            "starting Epoch 25\n",
            "Training...\n",
            "Train loss: 0.7170727891581399\n",
            "Train loss: 0.6926964398088127\n",
            "Train loss: 0.6853338927030563\n",
            "Train loss: 0.6813392083523637\n",
            "Train loss: 0.6786784569959383\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7099366060325077, Val f1: 0.6188837289810181\n",
            "Val loss: 0.6855486857480017, Val f1: 0.5987733006477356\n",
            "Val loss: 0.6778249334205281, Val f1: 0.5911207795143127\n",
            "Val loss: 0.6736898674803266, Val f1: 0.5887033343315125\n",
            "Val loss: 0.6712429604014835, Val f1: 0.5887755751609802\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8863858779271444, Val f1: 0.7744172215461731\n",
            "Val loss: 0.7586703726223537, Val f1: 0.6643150448799133\n",
            "Val loss: 0.7241113510998812, Val f1: 0.628909707069397\n",
            "Val loss: 0.7089663664499919, Val f1: 0.6108156442642212\n",
            "Val loss: 0.700377671342147, Val f1: 0.5978266000747681\n",
            "\n",
            "starting Epoch 26\n",
            "Training...\n",
            "Train loss: 0.7151934121336255\n",
            "Train loss: 0.6919481117149879\n",
            "Train loss: 0.6841509274461053\n",
            "Train loss: 0.6806121771618471\n",
            "Train loss: 0.6784684915800352\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7059575659888131, Val f1: 0.6746707558631897\n",
            "Val loss: 0.6828590076545189, Val f1: 0.6464930772781372\n",
            "Val loss: 0.6748997352340005, Val f1: 0.6402119994163513\n",
            "Val loss: 0.6712173152778108, Val f1: 0.6353393793106079\n",
            "Val loss: 0.6691209085889764, Val f1: 0.6310604214668274\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8841789563496908, Val f1: 0.8264943957328796\n",
            "Val loss: 0.7568296960421971, Val f1: 0.7058348059654236\n",
            "Val loss: 0.7225392677567222, Val f1: 0.6705949306488037\n",
            "Val loss: 0.7073638081550598, Val f1: 0.6515222787857056\n",
            "Val loss: 0.698671071152938, Val f1: 0.6384857892990112\n",
            "\n",
            "starting Epoch 27\n",
            "Training...\n",
            "Train loss: 0.7138081703867231\n",
            "Train loss: 0.6906464942570391\n",
            "Train loss: 0.6820673617449674\n",
            "Train loss: 0.6787570805872901\n",
            "Train loss: 0.676466344176112\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7054042858736855, Val f1: 0.6574371457099915\n",
            "Val loss: 0.681593471559985, Val f1: 0.6346996426582336\n",
            "Val loss: 0.6739000664515928, Val f1: 0.6262461543083191\n",
            "Val loss: 0.6701341948266757, Val f1: 0.6221985816955566\n",
            "Val loss: 0.6679554396384472, Val f1: 0.6198336482048035\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8823117613792419, Val f1: 0.8109369277954102\n",
            "Val loss: 0.7552470224244254, Val f1: 0.6915121674537659\n",
            "Val loss: 0.7209849953651428, Val f1: 0.6570824980735779\n",
            "Val loss: 0.7059610724449158, Val f1: 0.6393187642097473\n",
            "Val loss: 0.6973328025717485, Val f1: 0.6265113949775696\n",
            "\n",
            "starting Epoch 28\n",
            "Training...\n",
            "Train loss: 0.7116515934467316\n",
            "Train loss: 0.6879204326662524\n",
            "Train loss: 0.6806356825611808\n",
            "Train loss: 0.6761067812725648\n",
            "Train loss: 0.6741392241941916\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7012900795255389, Val f1: 0.6998245120048523\n",
            "Val loss: 0.6782137402172747, Val f1: 0.6700038909912109\n",
            "Val loss: 0.6706381060860374, Val f1: 0.6610031127929688\n",
            "Val loss: 0.6668160042520297, Val f1: 0.6567919254302979\n",
            "Val loss: 0.6646765349684535, Val f1: 0.6553136706352234\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8787721594174703, Val f1: 0.8530120849609375\n",
            "Val loss: 0.7523484740938459, Val f1: 0.7327404022216797\n",
            "Val loss: 0.7183131033724005, Val f1: 0.6941688060760498\n",
            "Val loss: 0.7033169627189636, Val f1: 0.6735323071479797\n",
            "Val loss: 0.6946958209338941, Val f1: 0.662065863609314\n",
            "\n",
            "starting Epoch 29\n",
            "Training...\n",
            "Train loss: 0.7124945606504168\n",
            "Train loss: 0.6875130965791899\n",
            "Train loss: 0.6790664412758567\n",
            "Train loss: 0.6758959273160514\n",
            "Train loss: 0.673373440632949\n",
            "\n",
            "Evaluating on train...\n",
            "Val loss: 0.7024228998592922, Val f1: 0.7093682885169983\n",
            "Val loss: 0.6768701343700804, Val f1: 0.6856637597084045\n",
            "Val loss: 0.6689118864861402, Val f1: 0.6785854697227478\n",
            "Val loss: 0.6650036391565355, Val f1: 0.6759142279624939\n",
            "Val loss: 0.6626940393770063, Val f1: 0.673237681388855\n",
            "\n",
            "Evaluating on test...\n",
            "Val loss: 0.8767628272374471, Val f1: 0.8737823367118835\n",
            "Val loss: 0.7507191215242658, Val f1: 0.7523461580276489\n",
            "Val loss: 0.7169198285449635, Val f1: 0.7148351073265076\n",
            "Val loss: 0.7018974542617797, Val f1: 0.6942742466926575\n",
            "Val loss: 0.6932378066213507, Val f1: 0.6839160323143005\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def predict(model, iterator):\n",
        "    model.eval()\n",
        "    fp = []\n",
        "    fn = []\n",
        "    tp = [] \n",
        "    tn = []\n",
        "    with torch.no_grad():\n",
        "        for i, (texts, ys) in tqdm(enumerate(iterator)):\n",
        "            preds = model(texts)  # делаем предсказания на тесте\n",
        "            for pred, gold, text in zip(preds, ys, texts):\n",
        "                text = ' '.join([id2symbol[int(word)] for word in text if word !=0])\n",
        "                if round(pred.item()) > gold:\n",
        "                    fp.append(text)\n",
        "                elif round(pred.item()) < gold:\n",
        "                    fn.append(text)\n",
        "                elif round(pred.item()) == gold == 1:\n",
        "                    tp.append(text)\n",
        "                elif round(pred.item()) == gold == 0:\n",
        "                    tn.append(text)\n",
        "    return fp, fn, tp, tn"
      ],
      "metadata": {
        "id": "ViyymkiTex0W"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fp, fn, tp, tn = predict(model, val_iterator)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0QEqT2hYex28",
        "outputId": "845ab231-69a0-46fd-d94a-4713b6beeb98"
      },
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "20it [00:05,  3.85it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (len(tp)+len(tn))/(len(fp) + len(fn) + len(tp) + len(tn))"
      ],
      "metadata": {
        "id": "8PBN9u61446i"
      },
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy) #размер эмбеддинга 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tfLTz2jaCaDL",
        "outputId": "3c0e5496-f77c-4e8d-86b4-82228cbccf3b"
      },
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6404\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy) #размер эмбеддинга 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BotLSWsg8iUE",
        "outputId": "93bbdd9f-1d54-4338-e9c0-446f5863dd84"
      },
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.64725\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy) #до изменений"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aRUL309b8VYC",
        "outputId": "d3729d2c-2a12-4188-983e-1b873049d338"
      },
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6909\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "recall = len(tp)/(len(tp) + len(fn))"
      ],
      "metadata": {
        "id": "ofk_nTuFxyFW"
      },
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(recall) #размер эмбеддинга 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yf7U-hNxCn3Q",
        "outputId": "972d7304-36ab-43f0-c11f-5bf96aae4c94"
      },
      "execution_count": 158,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6628184713375797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(recall) #размер эмбеддинга 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7eAhpAq8jK8",
        "outputId": "15b96538-1c36-4703-b5e4-6208bb7ba154"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7364649681528662\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(recall) #до изменений"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yLazcIbK8Ydf",
        "outputId": "abaf11ed-21f9-4f5e-b1c8-351466a32ae6"
      },
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6797372611464968\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "precision = len(tp)/(len(tp)+len(fp))"
      ],
      "metadata": {
        "id": "EPONUEyc4pqy"
      },
      "execution_count": 159,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision) #размер эмбеддинга 4"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1zsbI9PCt9Q",
        "outputId": "e40ff743-801c-411c-a201-10f8b6121a52"
      },
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6364678899082569\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision) #размер эмбеддинга 6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u00Y1hoG8lvV",
        "outputId": "e2e311e4-39d7-4072-fc70-44bccd5c31ef"
      },
      "execution_count": 149,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6267468450918946\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(precision) #до изменений"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AMcckllV8Z55",
        "outputId": "28b6d6d7-5d1d-4540-f115-151e4c358629"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6973657341229325\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Вторая модель"
      ],
      "metadata": {
        "id": "Hk-l125p3pdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_sym = Counter()\n",
        "for symbol in tweets_data['text']:\n",
        "    vocab_sym.update(list(symbol))\n",
        "\n",
        "filtered_vocab_sym = set()\n",
        "\n",
        "for symb in vocab_sym:\n",
        "    if vocab_sym[symb] > 5:\n",
        "        filtered_vocab_sym.add(symb)\n",
        "\n",
        "symb2id = {'PAD':0}\n",
        "\n",
        "for symb in filtered_vocab_sym:\n",
        "    symb2id[symb] = len(symb2id)\n",
        "    \n",
        "id2symbol = {i:symb for symb, i in symb2id.items()}"
      ],
      "metadata": {
        "id": "-ZOh1h4MywCK"
      },
      "execution_count": 167,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetsDataset2(Dataset):\n",
        "\n",
        "    def __init__(self, dataset, word2id, symb2id, DEVICE):\n",
        "        self.dataset = dataset['text'].values\n",
        "        self.word2id = word2id\n",
        "        self.symb2id = symb2id\n",
        "        self.length = dataset.shape[0]\n",
        "        self.target = torch.Tensor(dataset['tone'].values)\n",
        "        self.device = DEVICE\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        symbols = list(self.dataset[index])\n",
        "        symbols_ids = torch.LongTensor([self.symb2id[symb] for symb in symbols if symb in self.symb2id])\n",
        "        \n",
        "        tokens = self.dataset[index].split()\n",
        "        words_ids = torch.LongTensor([self.word2id[token] for token in tokens if token in self.word2id])\n",
        "        y = [self.target[index]]\n",
        "        \n",
        "        return words_ids, symbols_ids, y\n",
        "\n",
        "    def collate_fn(self, batch):\n",
        "        words_ids, symbols_ids, y = list(zip(*batch))\n",
        "        padded_sym_ids = pad_sequence(symbols_ids, batch_first=True).to(self.device)\n",
        "        padded_words_ids = pad_sequence(words_ids, batch_first=True).to(self.device)\n",
        "        y = torch.Tensor(y).to(self.device)\n",
        "        return padded_words_ids, padded_sym_ids, y"
      ],
      "metadata": {
        "id": "QtY4Sz8o4cvm"
      },
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TweetsDataset2(train_sentences, symbol2id, symb2id, DEVICE)\n",
        "train_sampler = RandomSampler(train_dataset)\n",
        "train_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)\n",
        "val_dataset = TweetsDataset2(val_sentences, symbol2id, symb2id, DEVICE)\n",
        "val_sampler = SequentialSampler(val_dataset)\n",
        "val_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)"
      ],
      "metadata": {
        "id": "73gbAuzf4cy7"
      },
      "execution_count": 165,
      "outputs": []
    }
  ]
}