{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"! pip install ipdb","metadata":{"id":"6gnva_Jk4cp-","outputId":"ef0d8c89-6f15-4807-bce1-d5ef3eafe386","execution":{"iopub.status.busy":"2021-12-21T19:00:37.745119Z","iopub.execute_input":"2021-12-21T19:00:37.745710Z","iopub.status.idle":"2021-12-21T19:00:49.392493Z","shell.execute_reply.started":"2021-12-21T19:00:37.745598Z","shell.execute_reply":"2021-12-21T19:00:49.391653Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"! pip install torchmetrics","metadata":{"id":"rjD0eBJtR7Bu","outputId":"376dce4f-4a65-43e5-c47b-26bf6c2e6e42","execution":{"iopub.status.busy":"2021-12-21T19:00:49.394757Z","iopub.execute_input":"2021-12-21T19:00:49.395090Z","iopub.status.idle":"2021-12-21T19:00:56.461714Z","shell.execute_reply.started":"2021-12-21T19:00:49.395049Z","shell.execute_reply":"2021-12-21T19:00:56.460777Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!wget https://www.dropbox.com/s/tn6x5f4ybaj34zf/Fake.csv?dl=0 -O data.csv","metadata":{"id":"9aY0ptGa94aY","outputId":"460c073c-930a-4d60-a2d1-46011eeebe31","execution":{"iopub.status.busy":"2021-12-21T19:00:56.463120Z","iopub.execute_input":"2021-12-21T19:00:56.463362Z","iopub.status.idle":"2021-12-21T19:01:05.618093Z","shell.execute_reply.started":"2021-12-21T19:00:56.463334Z","shell.execute_reply":"2021-12-21T19:01:05.617367Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom string import punctuation\nfrom collections import Counter\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.nn.utils.rnn import pad_sequence\nimport torch.optim as optim\nimport ipdb\n\nimport re\nimport torchmetrics\nfrom torchmetrics import F1\nfrom torchmetrics.functional import f1, recall\nimport matplotlib.pyplot as plt","metadata":{"id":"8-k_Aft6RwCK","execution":{"iopub.status.busy":"2021-12-21T19:01:05.620536Z","iopub.execute_input":"2021-12-21T19:01:05.621013Z","iopub.status.idle":"2021-12-21T19:01:08.512562Z","shell.execute_reply.started":"2021-12-21T19:01:05.620981Z","shell.execute_reply":"2021-12-21T19:01:08.511675Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"df = pd.read_csv('data.csv')","metadata":{"id":"8LcFO32G-CWn","execution":{"iopub.status.busy":"2021-12-21T19:01:08.513832Z","iopub.execute_input":"2021-12-21T19:01:08.514063Z","iopub.status.idle":"2021-12-21T19:01:09.207130Z","shell.execute_reply.started":"2021-12-21T19:01:08.514036Z","shell.execute_reply":"2021-12-21T19:01:09.206123Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"id":"ppZcfgo3P_p0","outputId":"35d9c224-146f-4c00-ee8c-dafb71e1b3d4","execution":{"iopub.status.busy":"2021-12-21T19:01:09.208493Z","iopub.execute_input":"2021-12-21T19:01:09.209057Z","iopub.status.idle":"2021-12-21T19:01:09.228642Z","shell.execute_reply.started":"2021-12-21T19:01:09.209020Z","shell.execute_reply":"2021-12-21T19:01:09.227855Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def preprocess(text):\n    res = re.sub(r'[^A-z]', ' ', text)\n    res = res.lower()\n    return res","metadata":{"id":"llrSBSXS4esr","execution":{"iopub.status.busy":"2021-12-21T19:01:09.229717Z","iopub.execute_input":"2021-12-21T19:01:09.229939Z","iopub.status.idle":"2021-12-21T19:01:09.234240Z","shell.execute_reply.started":"2021-12-21T19:01:09.229905Z","shell.execute_reply":"2021-12-21T19:01:09.233691Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"df['text'] = df.text.apply(preprocess)","metadata":{"id":"bK3mfYgZPpcU","execution":{"iopub.status.busy":"2021-12-21T19:01:09.235142Z","iopub.execute_input":"2021-12-21T19:01:09.235874Z","iopub.status.idle":"2021-12-21T19:01:12.066164Z","shell.execute_reply.started":"2021-12-21T19:01:09.235808Z","shell.execute_reply":"2021-12-21T19:01:12.065316Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"df[\"subject\"].value_counts()","metadata":{"id":"UuZhjnHaQIwC","outputId":"e7865e92-d505-46ff-e937-bbca0a505e47","execution":{"iopub.status.busy":"2021-12-21T19:01:12.067409Z","iopub.execute_input":"2021-12-21T19:01:12.067648Z","iopub.status.idle":"2021-12-21T19:01:12.078455Z","shell.execute_reply.started":"2021-12-21T19:01:12.067609Z","shell.execute_reply":"2021-12-21T19:01:12.077590Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"coded = {\"subject\":     {\"News\": 0, \n                                \"politics\": 1,\n                                \"left-news\": 2,\n                                \"Government News\": 3,\n                                \"US_News\": 4,\n                                \"Middle-east\": 5}}","metadata":{"id":"fFlfRI2xpl9Y","execution":{"iopub.status.busy":"2021-12-21T19:01:12.082049Z","iopub.execute_input":"2021-12-21T19:01:12.082510Z","iopub.status.idle":"2021-12-21T19:01:12.089360Z","shell.execute_reply.started":"2021-12-21T19:01:12.082477Z","shell.execute_reply":"2021-12-21T19:01:12.088648Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"df = df.replace(coded)","metadata":{"id":"AUxH5wKDPnKv","execution":{"iopub.status.busy":"2021-12-21T19:01:12.090366Z","iopub.execute_input":"2021-12-21T19:01:12.090569Z","iopub.status.idle":"2021-12-21T19:01:12.117069Z","shell.execute_reply.started":"2021-12-21T19:01:12.090540Z","shell.execute_reply":"2021-12-21T19:01:12.116462Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"df.head(3)","metadata":{"id":"b9ydTwdEGjJB","outputId":"3647e4c7-4570-4fcc-8151-dcae14a21a45","execution":{"iopub.status.busy":"2021-12-21T19:01:12.118157Z","iopub.execute_input":"2021-12-21T19:01:12.118718Z","iopub.status.idle":"2021-12-21T19:01:12.128628Z","shell.execute_reply.started":"2021-12-21T19:01:12.118686Z","shell.execute_reply":"2021-12-21T19:01:12.127899Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_sentences, val_sentences = train_test_split(df, test_size=0.2)","metadata":{"id":"t_F8Va0BRQq6","execution":{"iopub.status.busy":"2021-12-21T19:01:12.129707Z","iopub.execute_input":"2021-12-21T19:01:12.129999Z","iopub.status.idle":"2021-12-21T19:01:12.142008Z","shell.execute_reply.started":"2021-12-21T19:01:12.129974Z","shell.execute_reply":"2021-12-21T19:01:12.141204Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"vocab = Counter()\n\nfor text in df['text']:\n    text = text.split()\n    vocab.update(text)\n#print('всего уникальных токенов:', len(vocab))\n\nfiltered_vocab = set()\nfor word in vocab:\n    if vocab[word] > 5:\n        filtered_vocab.add(word)\n#print('уникальных токенов, встретившихся больше 5 раз:', len(filtered_vocab))","metadata":{"id":"F7EZLNVURVZM","execution":{"iopub.status.busy":"2021-12-21T19:01:12.143122Z","iopub.execute_input":"2021-12-21T19:01:12.143751Z","iopub.status.idle":"2021-12-21T19:01:14.170852Z","shell.execute_reply.started":"2021-12-21T19:01:12.143717Z","shell.execute_reply":"2021-12-21T19:01:14.169966Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"word2id = {'PAD':0}\n\nfor word in filtered_vocab:\n    word2id[word] = len(word2id)\n\n#обратный словарь для того, чтобы раскодировать последовательность\nid2word = {i:word for word, i in word2id.items()}","metadata":{"id":"Hb-bZPX-RVbu","execution":{"iopub.status.busy":"2021-12-21T19:01:14.171988Z","iopub.execute_input":"2021-12-21T19:01:14.172211Z","iopub.status.idle":"2021-12-21T19:01:14.193997Z","shell.execute_reply.started":"2021-12-21T19:01:14.172184Z","shell.execute_reply":"2021-12-21T19:01:14.193147Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"DEVICE = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nDEVICE","metadata":{"id":"dsajyJ6iD1gH","outputId":"c15a3c65-d94d-4688-fd32-5071923985ae","execution":{"iopub.status.busy":"2021-12-21T19:01:14.195272Z","iopub.execute_input":"2021-12-21T19:01:14.195952Z","iopub.status.idle":"2021-12-21T19:01:14.201639Z","shell.execute_reply.started":"2021-12-21T19:01:14.195915Z","shell.execute_reply":"2021-12-21T19:01:14.200902Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, dataset, col, target_col, word2id, max_len, DEVICE):\n        self.dataset = dataset[col].values\n        self.word2id = word2id\n        self.length = dataset.shape[0]\n        self.target = torch.Tensor(dataset[target_col].values)\n        self.max_len = max_len\n        self.device = DEVICE\n    \n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, index): \n        tokens = self.dataset[index].split()\n        ids = torch.LongTensor([self.word2id[token] if token in self.word2id else self.word2id['PAD'] for token in tokens][:self.max_len])\n        y = [self.target[index]]\n        return ids, y\n\n    def collate_fn(self, batch):\n      ids, y = list(zip(*batch))\n      padded_ids = torch.vstack([F.pad(seq, pad=(0, self.max_len - seq.shape[0]), mode='constant', value=0) for seq in ids])\n      padded_ids = pad_sequence(ids, batch_first=True).to(self.device)\n      y = torch.LongTensor(y).to(self.device)\n      return padded_ids, y.T[0]","metadata":{"id":"wXI-CTJsPnNL","execution":{"iopub.status.busy":"2021-12-21T19:01:14.203003Z","iopub.execute_input":"2021-12-21T19:01:14.203427Z","iopub.status.idle":"2021-12-21T19:01:14.215494Z","shell.execute_reply.started":"2021-12-21T19:01:14.203385Z","shell.execute_reply":"2021-12-21T19:01:14.214969Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(train_sentences, 'text', 'subject', word2id, 400, DEVICE)\ntrain_sampler = RandomSampler(train_dataset)\ntrain_iterator = DataLoader(train_dataset, collate_fn = train_dataset.collate_fn, sampler=train_sampler, batch_size=1024)","metadata":{"id":"FnX3gKIeXM81","execution":{"iopub.status.busy":"2021-12-21T19:01:14.216776Z","iopub.execute_input":"2021-12-21T19:01:14.217140Z","iopub.status.idle":"2021-12-21T19:01:14.245877Z","shell.execute_reply.started":"2021-12-21T19:01:14.217101Z","shell.execute_reply":"2021-12-21T19:01:14.245252Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"val_dataset = Dataset(val_sentences, 'text', 'subject', word2id, 400, DEVICE)\nval_sampler = SequentialSampler(val_dataset)\nval_iterator = DataLoader(val_dataset, collate_fn = val_dataset.collate_fn, sampler=val_sampler, batch_size=1024)","metadata":{"id":"C3g-bEuMDBHq","execution":{"iopub.status.busy":"2021-12-21T19:01:14.247183Z","iopub.execute_input":"2021-12-21T19:01:14.247659Z","iopub.status.idle":"2021-12-21T19:01:14.253574Z","shell.execute_reply.started":"2021-12-21T19:01:14.247590Z","shell.execute_reply":"2021-12-21T19:01:14.252831Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"! wget -c \"https://s3.amazonaws.com/dl4j-distribution/GoogleNews-vectors-negative300.bin.gz\"","metadata":{"id":"gZmwnjcVQ5kj","outputId":"8fb7a0fb-ab0e-42fc-fbf0-a04bc7d75a17","execution":{"iopub.status.busy":"2021-12-21T19:01:14.254715Z","iopub.execute_input":"2021-12-21T19:01:14.255159Z","iopub.status.idle":"2021-12-21T19:03:38.844796Z","shell.execute_reply.started":"2021-12-21T19:01:14.255117Z","shell.execute_reply":"2021-12-21T19:03:38.843718Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import gensim\n\nw2v = gensim.models.KeyedVectors.load_word2vec_format(\"GoogleNews-vectors-negative300.bin.gz\", binary=True)","metadata":{"id":"4TlCdg3vU2j4","execution":{"iopub.status.busy":"2021-12-21T19:03:38.848293Z","iopub.execute_input":"2021-12-21T19:03:38.848531Z","iopub.status.idle":"2021-12-21T19:04:37.911522Z","shell.execute_reply.started":"2021-12-21T19:03:38.848502Z","shell.execute_reply":"2021-12-21T19:04:37.910742Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"weights = np.zeros((len(word2id), 300))\n\nfor word, i in word2id.items():\n    try:\n        weights[i] = w2v[word]\n    except KeyError:\n        weights[i] = np.random.uniform(-0.25, 0.25, 300)\n\nweights = torch.FloatTensor(weights)","metadata":{"id":"YlFOqXd0U2nS","execution":{"iopub.status.busy":"2021-12-21T19:04:37.913480Z","iopub.execute_input":"2021-12-21T19:04:37.914262Z","iopub.status.idle":"2021-12-21T19:04:38.150884Z","shell.execute_reply.started":"2021-12-21T19:04:37.914211Z","shell.execute_reply":"2021-12-21T19:04:38.150049Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"Использовала код на tensorflow отсюда: https://github.com/KifayatMsd/C-LSTM-text-classification/blob/master/clstm_classifier.py","metadata":{"id":"mmBHXVVvuxI0"}},{"cell_type":"code","source":"class clstm_clf(nn.Module):\n  def __init__(self, max_length, vocab_size, filter_list, drop_first):\n    super().__init__()\n    self.vocab_size = vocab_size\n    self.filter_list = filter_list\n    self.drop_first = drop_first\n    self.max_length = max_length\n    \n    self.embedding = nn.Embedding(vocab_size, 300)\n    self.embedding.from_pretrained(torch.tensor(weights))\n    \n    self.conv2 = nn.Conv1d(in_channels=300, out_channels=150, kernel_size=2)\n    self.conv3 = nn.Conv1d(in_channels=300, out_channels=150, kernel_size=3)\n    self.conv4 = nn.Conv1d(in_channels=300, out_channels=150, kernel_size=4)\n\n    self.convs = [self.conv2, self.conv3, self.conv4]\n    \n    self.lstm = nn.LSTM(input_size=150, hidden_size=150, num_layers=1, batch_first=True)\n    self.dropout = nn.Dropout(p=0.5)            \n    self.relu = nn.ReLU()\n    self.out = nn.Softmax(dim=1)\n    self.linear = nn.Linear(150, 6)\n\n  def forward(self, text):\n    embedded = self.dropout(self.embedding(text)).transpose(1, 2)\n    max_len = self.max_length - max(self.filter_list) + 1\n    \n    outputs = []\n    for item in range(2):\n      layer = self.convs[item](embedded)\n      hid = self.relu(layer)[:, :, :max_len]\n      outputs.append(hid)\n\n    if len(self.filter_list) > 1:\n      rnn_inputs = torch.cat(outputs, -1)\n    else:\n      rnn_inputs = hid\n\n    _, (hidden_state, _) = self.lstm(rnn_inputs.transpose(1, 2))\n    if self.drop_first:\n      embedded = self.dropout(embedded)\n    logits = self.out(self.linear(torch.squeeze(hidden_state, 0)))\n        \n    return logits","metadata":{"id":"6ploz_mDpl9h","execution":{"iopub.status.busy":"2021-12-21T19:04:38.151907Z","iopub.execute_input":"2021-12-21T19:04:38.152426Z","iopub.status.idle":"2021-12-21T19:04:38.212943Z","shell.execute_reply.started":"2021-12-21T19:04:38.152394Z","shell.execute_reply":"2021-12-21T19:04:38.212064Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def train(model, iterator, optimizer, criterion):\n    epoch_loss = 0 # для подсчета среднего лосса на всех батчах\n\n    model.train()  # ставим модель в обучение, явно указываем, что сейчас надо будет хранить градиенты у всех весов\n\n    for i, (texts, ys) in enumerate(iterator): #итерируемся по батчам\n        optimizer.zero_grad()  #обнуляем градиенты\n        preds = model(texts)  #прогоняем данные через модель\n        loss = criterion(preds, ys) #считаем значение функции потерь  \n        loss.backward() #считаем градиенты  \n        optimizer.step() #обновляем веса \n        epoch_loss += loss.item() #сохраняем значение функции потерь\n        if not (i + 1) % int(len(iterator)/5):\n            print(f'Train loss: {epoch_loss/i}')      \n    return  epoch_loss / len(iterator) # возвращаем среднее значение лосса по всей выборке","metadata":{"id":"aRHg5Gw0W30q","execution":{"iopub.status.busy":"2021-12-21T19:04:38.214190Z","iopub.execute_input":"2021-12-21T19:04:38.214571Z","iopub.status.idle":"2021-12-21T19:04:38.223566Z","shell.execute_reply.started":"2021-12-21T19:04:38.214542Z","shell.execute_reply":"2021-12-21T19:04:38.222826Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"def evaluate(model, iterator, criterion):\n    epoch_loss = 0\n    epoch_metric = 0\n    model.eval() \n    with torch.no_grad():\n        for i, (texts, ys) in enumerate(iterator):   \n            preds = model(texts)  # делаем предсказания на тесте\n            loss = criterion(preds, ys)   # считаем значения функции ошибки для статистики  \n            epoch_loss += loss.item()\n            batch_metric = f1(preds.argmax(1).long(), ys.long(), ignore_index=0)\n            epoch_metric += batch_metric\n            if i != 0:\n                if not (i + 1) % int(len(iterator)/5):\n                  #print(f'длина итератора: ', len(iterator))\n                  print(f'Val loss: {epoch_loss/i}, Val f1: {epoch_metric/i}')\n        \n    return epoch_metric / len(iterator), epoch_loss / len(iterator) # возвращаем среднее значение по всей выборке","metadata":{"id":"HdUNzMX3W34O","execution":{"iopub.status.busy":"2021-12-21T19:04:38.225001Z","iopub.execute_input":"2021-12-21T19:04:38.225463Z","iopub.status.idle":"2021-12-21T19:04:38.236173Z","shell.execute_reply.started":"2021-12-21T19:04:38.225433Z","shell.execute_reply":"2021-12-21T19:04:38.235355Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"def learning(n_epochs, model, optimizer, criterion):\n    for i in range(n_epochs):\n        print(f'\\nstarting Epoch {i}')\n        print('Training...')\n        epoch_loss = train(model, train_iterator, optimizer, criterion)\n        losses.append(epoch_loss)\n        print('\\nEvaluating on train...')\n        f1_on_train,_ = evaluate(model, train_iterator, criterion)\n        f1s.append(f1_on_train.cpu())\n        print('\\nEvaluating on test...')\n        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n        losses_eval.append(epoch_loss_on_test)\n        f1s_eval.append(f1_on_test.cpu())","metadata":{"id":"8OInb2In1M-9","execution":{"iopub.status.busy":"2021-12-21T19:04:38.238850Z","iopub.execute_input":"2021-12-21T19:04:38.239156Z","iopub.status.idle":"2021-12-21T19:04:38.248094Z","shell.execute_reply.started":"2021-12-21T19:04:38.239118Z","shell.execute_reply":"2021-12-21T19:04:38.247560Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"model = clstm_clf(max_length=400, vocab_size=len(word2id), filter_list=[2], drop_first=0)\noptimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\ncriterion = nn.CrossEntropyLoss()\ncriterion(torch.tensor([0.45, 0.22, 0.32]).unsqueeze(1).unsqueeze(0), torch.tensor([1]).unsqueeze(0).long())\n","metadata":{"id":"wU2sQCSdpl9j","outputId":"1b916564-8cee-4d2c-cce5-5b9d75fe8bbd","execution":{"iopub.status.busy":"2021-12-21T19:04:38.249287Z","iopub.execute_input":"2021-12-21T19:04:38.250865Z","iopub.status.idle":"2021-12-21T19:04:38.446601Z","shell.execute_reply.started":"2021-12-21T19:04:38.250792Z","shell.execute_reply":"2021-12-21T19:04:38.445693Z"},"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"losses = []\nlosses_eval = []\nf1s = []\nf1s_eval = []\n\nfor i in range(5):\n    print(f'\\nstarting Epoch {i}')\n    print('Training...')\n    epoch_loss = train(model, train_iterator, optimizer, criterion)\n    losses.append(epoch_loss)\n    print('\\nEvaluating on train...')\n    f1_on_train,_ = evaluate(model, train_iterator, criterion)\n    f1s.append(f1_on_train)\n    print('\\nEvaluating on test...')\n    f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n    losses_eval.append(epoch_loss_on_test)\n    f1s_eval.append(f1_on_test)","metadata":{"id":"ZnQXEd_o1NBO","outputId":"e0af865a-5de5-4e1d-d3a5-8d97dbf37827","execution":{"iopub.status.busy":"2021-12-21T19:04:38.451517Z","iopub.execute_input":"2021-12-21T19:04:38.451904Z","iopub.status.idle":"2021-12-21T19:40:46.586144Z","shell.execute_reply.started":"2021-12-21T19:04:38.451861Z","shell.execute_reply":"2021-12-21T19:40:46.585361Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":" Подбор гиперпараметров","metadata":{"id":"g9leTKUFOrPB"}},{"cell_type":"code","source":"filters = [[2], [3]]\ndrop_first = [0, 1]\nbest_f1 = 0\n\nfor filter in filters:\n  for d in drop_first:\n    losses_2 = []\n    losses_eval_2 = []\n    f1s_2 = []\n    f1s_eval_2 = []\n    for i in range(5):\n        print('filter= ' + str(filter) + 'drop_first= ' + str(drop_first))\n        model = clstm_clf(max_length=400, vocab_size=len(word2id), filter_list=filter, drop_first=d)\n        optimizer = torch.optim.RMSprop(model.parameters(), lr=0.001, weight_decay=0.0001)\n        criterion = nn.CrossEntropyLoss()\n        print(f'\\nstarting Epoch {i}')\n        print('Training...')\n        epoch_loss = train(model, train_iterator, optimizer, criterion)\n        losses_2.append(epoch_loss)\n        print('\\nEvaluating on train...')\n        f1_on_train,_ = evaluate(model, train_iterator, criterion)\n        f1s_2.append(f1_on_train)\n        print('\\nEvaluating on test...')\n        f1_on_test, epoch_loss_on_test = evaluate(model, val_iterator, criterion)\n        losses_eval.append(epoch_loss_on_test)\n        f1s_eval_2.append(f1_on_test)\n        if f1_on_test > best_f1:\n          best_f1 = f1_on_test\n          print('new best_f1 reached')","metadata":{"id":"P9OBlYwRLRsX","outputId":"a89ec99e-44a6-49eb-e528-aea9fff1c991","execution":{"iopub.status.busy":"2021-12-21T19:40:46.587570Z","iopub.execute_input":"2021-12-21T19:40:46.588359Z","iopub.status.idle":"2021-12-21T22:56:41.292725Z","shell.execute_reply.started":"2021-12-21T19:40:46.588318Z","shell.execute_reply":"2021-12-21T22:56:41.291277Z"},"trusted":true},"execution_count":29,"outputs":[]},{"cell_type":"markdown","source":"Наилучшее качество достигнуто с фильтром [2] и drop_first 0. В целом качество модели не очень хорошее, возможно, потому что было мало эпох для обучения (простите, больше колаб не тянул и вырубался). Можно бы было еще поэкспериментировать с гиперпараметрами, но памяти хватило только на такое. Но вообще видно, что модель чему-то да учится (лосс уменьшается, качество растет), хотя и не очень успешно (прям как я на этом курсе). ","metadata":{}}]}